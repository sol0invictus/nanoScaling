# SFT Configuration
out_dir: "out-sft"
eval_interval: 20
log_interval: 1
eval_iters: 10
eval_only: False
always_save_checkpoint: True
init_from: "scratch" # 'gpt2' for real usage

dataset: "sft_dummy"
gradient_accumulation_steps: 1
batch_size: 4
block_size: 1024

# Model (Small for testing)
n_layer: 4
n_head: 4
n_embd: 128
dropout: 0.1

learning_rate: 1e-4
max_iters: 100
warmup_iters: 10
lr_decay_iters: 100
min_lr: 1e-5

device: "cuda"
compile: False # Faster start for debugging
